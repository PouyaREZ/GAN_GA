{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readme\n",
    "+ Section 1 of this notebook trains the GAN on the optimization solutions, and outputs the initial solutions using the generator. Run section 1 after inputting the name of the experiment you would like to run at the beginning of section 1.2.\n",
    "    + The generated raw solutions along with the plots of accuracy and loss are stored in ./CGAN/images\n",
    "    + The input data for the selected experiment is stored in ./CGAN/input_data/\n",
    "    + The final TF model is stored in ./CGAN/saved_model/\n",
    "\n",
    "+ Move the data you like to analyze from ./CGAN/images/ to ./Data/\n",
    "\n",
    "+ Section 2 of this notebook runs the evaluation function on the generated solutions, and outputs the stats and metrics associated with the generated solutions. Run section 2 after:\n",
    "    + determining the name(s) of the experiment(s) and the configurations in lines 3-9 of section 2.3 for running the evaluation function,\n",
    "    + determining the name(s) of the experiment(s) and the configurations in lines 15-17 of section 2.4 for calculating the hypervolumes, and\n",
    "    + making sure the filenames listed in lines 2-15 of section 2.2 are those you have moved from ./CGAN/images/ to ./Data/.\n",
    "    \n",
    "+ You can find the complementary solutions in ./Data/ and the plots in ./Figures/. The rest of the stats are printed to the console."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Train the C-GAN and output the initial solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MmYUxk2MZSja"
   },
   "source": [
    "## 1.1 Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ez8_vkV0WlG0"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Taken from https://github.com/eriklindernoren/Keras-GAN/tree/master/cgan\n",
    "\n",
    "Modified by Author\n",
    "Last modified on 1/22/2021\n",
    "'''\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, Concatenate\n",
    "from keras.layers import BatchNormalization, Activation\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5MC6mSafXY_v"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('./CGAN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HG99WiGnh_Z6"
   },
   "source": [
    "## 1.2 Main (Input Data and Train CGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5JZqyQKko8A"
   },
   "outputs": [],
   "source": [
    "#  Set a name to distinguish the generated logs, visualizations, and models\n",
    "run_name = 'WorstHalfAll'\n",
    "# Determine the name of the experiment you want to run\n",
    "experiment = 'WorstHalfAll'\n",
    "# Determine if you want the long runs or short runs\n",
    "long_run = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5D7aK03ePP9"
   },
   "source": [
    "### 1.2.1 Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2549,
     "status": "ok",
     "timestamp": 1610065661631,
     "user": {
      "displayName": "Pouya Rezazadeh Kalehbasti",
      "photoUrl": "",
      "userId": "12310598957606952973"
     },
     "user_tz": 480
    },
    "id": "BV3agAkzeOfE",
    "outputId": "295a1f15-b768-40f7-cb34-085835e8b4f7"
   },
   "outputs": [],
   "source": [
    "# Constants for loading the dataset\n",
    "Num_Sites = 4\n",
    "\n",
    "LCC_Var = Num_Sites+11\n",
    "CO2_Var = Num_Sites+14\n",
    "WalkScore_Var = Num_Sites+15\n",
    "GFA_Var = Num_Sites+16\n",
    "FAR_Var = Num_Sites+17\n",
    "\n",
    "PercentArea_0 = Num_Sites+19\n",
    "PercentArea_1 = Num_Sites+27\n",
    "\n",
    "Max_FAR = 20\n",
    "Max_Site_GFA = 647497/Max_FAR # m2\n",
    "\n",
    "error_tol = 1.15\n",
    "\n",
    "\n",
    "# Import and filter the file `filename` based on several conditions\n",
    "def DF_Filter(filename): # Similar to the one in Plots_Paper_One.py               \n",
    "    file = np.loadtxt(filename, dtype='float', delimiter=',')\n",
    "    inputDF = pd.DataFrame(file)\n",
    "\n",
    "    \n",
    "    \n",
    "    print('+++++ processing %s +++++\\n'%(filename))\n",
    "    \n",
    "    print('Count duplicates:')\n",
    "    condition1 = inputDF.duplicated()==True\n",
    "    print(inputDF[condition1][GFA_Var].count())\n",
    "    \n",
    "    print('Count under the min GFA:') # Count non-trivial neighborhoods\n",
    "    condition2 = inputDF[GFA_Var] <= 1/error_tol#<=647497/10\n",
    "    print(inputDF[condition2][GFA_Var].count())\n",
    "    \n",
    "    print('Count over the max GFA:')\n",
    "    condition3 = inputDF[GFA_Var] >= Max_Site_GFA*Max_FAR*error_tol\n",
    "    print(inputDF[condition3][GFA_Var].count())\n",
    "    \n",
    "    print('Count over the max Site GFA:')\n",
    "    condition4 = inputDF[GFA_Var]/inputDF[FAR_Var] >= Max_Site_GFA*error_tol\n",
    "    print(inputDF[condition4][GFA_Var].count())\n",
    "\n",
    "\n",
    "    # Normalizing the LCC and CO2 objectives\n",
    "    print('Normalizing the LCC and CO2 Obj Fxns')\n",
    "    inputDF[LCC_Var] /= inputDF[GFA_Var] # Normalizing LCC ($/m2)\n",
    "    inputDF[CO2_Var] /= inputDF[GFA_Var] # Normalizing CO2 (Tonnes/m2)\n",
    "    print('Converting percent areas from decimal into percentage')\n",
    "    for i in range(PercentArea_0, PercentArea_1+1): # Converting percent areas to integer %\n",
    "        inputDF[i] = inputDF[i] * 100\n",
    "\n",
    "\n",
    "    # Filter the data based on the 50-th percentiles of the objective function values\n",
    "    if experiment == 'WorstHalfLCC':\n",
    "        print('Permit only the worst 50% of individuals in terms of LCC:')\n",
    "        LCC_percentile = np.nanpercentile(inputDF[LCC_Var], 50)\n",
    "        print(\"Lowest LCC considered in training data:%.2f\"%LCC_percentile)\n",
    "        condition8 = (inputDF[LCC_Var] >= LCC_percentile)\n",
    "\n",
    "    elif experiment == 'BestHalfLCC':\n",
    "        print('Permit only the best 50% of individuals in terms of LCC:')\n",
    "        LCC_percentile = np.nanpercentile(inputDF[LCC_Var], 50)\n",
    "        print(\"Highest LCC considered in training data:%.2f\"%LCC_percentile)\n",
    "        condition8 = (inputDF[LCC_Var] <= LCC_percentile)\n",
    "        \n",
    "    elif experiment == 'WorstHalfCO2':\n",
    "        print('Permit only the worst 50% of individuals in terms of CO2:')\n",
    "        CO2_percentile = np.nanpercentile(inputDF[CO2_Var], 50)\n",
    "        print(\"Lowest CO2 considered in training data: %.2f\"%CO2_percentile)\n",
    "        condition8 = (inputDF[CO2_Var] >= CO2_percentile)\n",
    "        \n",
    "    elif experiment == 'WorstHalfWalkScore':\n",
    "        print('Permit only the worst 50% of individuals in terms of WalkScore:')\n",
    "        WalkScore_percentile = np.nanpercentile(inputDF[WalkScore_Var], 50)\n",
    "        print(\"Highest Walkscore considered in training data:%.2f\"%WalkScore_percentile)\n",
    "        condition8 = (inputDF[WalkScore_Var] <= WalkScore_percentile)\n",
    "\n",
    "    elif experiment == 'WorstHalfAll':\n",
    "        print('Permit only the worst 50% of individuals in terms of all objective functions:')\n",
    "        LCC_percentile = np.nanpercentile(inputDF[LCC_Var], 50)\n",
    "        CO2_percentile = np.nanpercentile(inputDF[CO2_Var], 50)\n",
    "        WalkScore_percentile = np.nanpercentile(inputDF[WalkScore_Var], 50)\n",
    "        condition8 = (inputDF[LCC_Var] >= LCC_percentile) & (inputDF[CO2_Var] >= CO2_percentile) & (inputDF[WalkScore_Var] <= WalkScore_percentile)\n",
    "    \n",
    "    elif experiment == 'BestHalfAll':\n",
    "        print('Permit only the best 50% of individuals in terms of all objective functions:')\n",
    "        LCC_percentile = np.nanpercentile(inputDF[LCC_Var], 50)\n",
    "        CO2_percentile = np.nanpercentile(inputDF[CO2_Var], 50)\n",
    "        WalkScore_percentile = np.nanpercentile(inputDF[WalkScore_Var], 50)\n",
    "        condition8 = (inputDF[LCC_Var] <= LCC_percentile) & (inputDF[CO2_Var] <= CO2_percentile) & (inputDF[WalkScore_Var] >= WalkScore_percentile)\n",
    "\n",
    "\n",
    "    elif experiment == 'BestHalfAny':\n",
    "        print('Permit only the best 50% of individuals in terms of each objective functions:')\n",
    "        LCC_percentile = np.nanpercentile(inputDF[LCC_Var], 50)\n",
    "        CO2_percentile = np.nanpercentile(inputDF[CO2_Var], 50)\n",
    "        WalkScore_percentile = np.nanpercentile(inputDF[WalkScore_Var], 50)\n",
    "        condition8 = (inputDF[LCC_Var] <= LCC_percentile) | (inputDF[CO2_Var] <= CO2_percentile) | (inputDF[WalkScore_Var] >= WalkScore_percentile)\n",
    "\n",
    "\n",
    "\n",
    "    # Filtering the inadmissible results\n",
    "    Filtered = ~(condition1 | condition2 | condition3 | condition4)\n",
    "    if experiment == 'FullData':\n",
    "      inputDF = inputDF[Filtered]\n",
    "    else:\n",
    "      inputDF = inputDF[Filtered & condition8]\n",
    "\n",
    "\n",
    "    if experiment == 'WorstHalfAll' or experiment == 'BestHalfAll' or experiment == 'BestHalfAny':\n",
    "        print(\"Lowest LCC considered in training data:%.2f\"%np.min(inputDF[LCC_Var]))\n",
    "        print(\"Lowest CO2 considered in training data: %.2f\"%np.min(inputDF[CO2_Var]))\n",
    "        print(\"Highest Walkscore considered in training data:%.2f\"%np.max(inputDF[WalkScore_Var]))    \n",
    "\n",
    "\n",
    "    \n",
    "    print('Count of valid answers: %d out of %d'%(len(inputDF), len(file)))\n",
    "    inputDF.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    return inputDF\n",
    "\n",
    "\n",
    "# Convert the raw input data into training features and labels vectors\n",
    "def load_data():\n",
    "    ## IMPORT DATA       \n",
    "    print('loading data')\n",
    "    filenames = ['../IILP_Toy_Optimization_TestRuns.txt']\n",
    "    # DFNames = ['CCHP+Network']\n",
    "    DF = DF_Filter(filenames[0])\n",
    "\n",
    "    ## Set the range of input variables for the NN, then import the train variables\n",
    "    inputRange = list(range(Num_Sites+8+1))\n",
    "    inputRange.remove(Num_Sites) # plant location: the building type at plant location is already Num_Buildings + 1\n",
    "    inputRange.remove(Num_Sites+3) # 0\n",
    "    inputRange.remove(Num_Sites+4) # 0\n",
    "    # Define X\n",
    "    X = DF.iloc[:,inputRange]\n",
    "    # Define y\n",
    "    outputRange = [LCC_Var, CO2_Var, WalkScore_Var] # THESE ARE NORMALIZED IN DF_Filter\n",
    "    y = DF.iloc[:, outputRange]\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    print('Scaling data')\n",
    "    # Scale X\n",
    "    x = X.values #returns a numpy array\n",
    "    std_scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "    X_Scaler = std_scaler.fit(x)\n",
    "    X = X_Scaler.transform(x)\n",
    "    # X = std_scaler.fit_transform(x)    \n",
    "    \n",
    "    # Scale y\n",
    "    y_values = y.values #returns a numpy array\n",
    "    std_scaler2 = preprocessing.MinMaxScaler(feature_range=(-1, 1))\n",
    "    y_Scaler = std_scaler2.fit(y_values)\n",
    "    y = y_Scaler.transform(y_values)\n",
    "\n",
    "    # Experiment: not scaling y: FAILED\n",
    "\n",
    "    X_test = None\n",
    "    y_test = None\n",
    "    X_train = X\n",
    "    y_train = y\n",
    "    \n",
    "    ## Reshape the data to fit the NN\n",
    "    X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1).astype('float32')\n",
    "    y_train = y_train.reshape(y_train.shape[0], y_train.shape[1], 1).astype('float32')\n",
    "    print('Scaled data')\n",
    "\n",
    "\n",
    "    print('Saved training data in current directory as %s'%(run_name+'.txt'))\n",
    "    np.savetxt('input_data/'+run_name+'.txt', DF.iloc[:, inputRange+outputRange])\n",
    "    return X_train, y_train, X_test, y_test, X_Scaler, y_Scaler\n",
    "\n",
    "\n",
    "X_train, y_train, X_test, y_test, X_Scaler, y_Scaler = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AaJmKM6Kfeh-"
   },
   "source": [
    "### 1.2.2 Create and train the CGAN on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iEzF0KeEW-bQ"
   },
   "outputs": [],
   "source": [
    "class CGAN():\n",
    "    def __init__(self, X_train, y_train, X_test, y_test, X_Scaler, y_Scaler, latent_dim=3):\n",
    "        # Input shape\n",
    "        # self.img_rows = 1\n",
    "        self.img_cols = len(X_train[0]) # Was 11 before Nov 5, 2020\n",
    "        self.label_size = len(y_train[0])\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_cols, self.channels) #(self.img_rows, self.img_cols, self.channels)\n",
    "        # self.num_classes = 10\n",
    "        self.latent_dim = latent_dim # Was 22\n",
    "\n",
    "\n",
    "        optimizer = Adam(learning_rate = 0.0002, beta_1 = 0.5, beta_2 = 0.999) # Adam(0.0002, 0.5)\n",
    "\n",
    "\n",
    "        self.X_train, self.y_train, self.X_Scaler, self.y_Scaler = \\\n",
    "            X_train, y_train, X_Scaler, y_Scaler\n",
    "\n",
    "\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build the generator\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        # The generator takes noise and the target label as input\n",
    "        # and generates the corresponding digit of that label\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(self.label_size,))\n",
    "        img = self.generator([noise, label])\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated image as input and determines validity\n",
    "        # and the label of that image\n",
    "        valid = self.discriminator([img, label])\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator)\n",
    "        # Trains generator to fool discriminator\n",
    "        self.combined = Model([noise, label], valid)\n",
    "        self.combined.compile(loss=['binary_crossentropy'],\n",
    "            optimizer=optimizer)\n",
    "\n",
    "\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        # Best architecture: 1. 128-64-32 2. 256-256-128, 3. 512-256-128; 4. 512-512-512\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128, input_dim=np.prod(self.img_shape)+self.label_size))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dense(64))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(32))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        label = Input(shape=(self.label_size,), dtype='float32')\n",
    "\n",
    "        # label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
    "        flat_img = Flatten()(img)\n",
    "\n",
    "        # model_input = multiply([flat_img, label_embedding])\n",
    "        model_input = Concatenate(axis=1)([flat_img, label])\n",
    "\n",
    "        validity = model(model_input)\n",
    "\n",
    "        return Model([img, label], validity)\n",
    "\n",
    "\n",
    "\n",
    "    def build_generator(self):\n",
    "      # Best architecture so far:\n",
    "      # 1. 64-128-64; 2. 128-256-128; 3. 256 - 512 - 256\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(64, input_dim=self.latent_dim+self.label_size))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(128))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(64))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Dense(np.prod(self.img_shape), activation='tanh'))\n",
    "        # model.add(Reshape(self.img_shape))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(self.label_size,), dtype='float32')\n",
    "        # label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
    "\n",
    "        # model_input = multiply([noise, label_embedding])\n",
    "        model_input = Concatenate(axis=1)([noise, label])\n",
    "        img = model(model_input)\n",
    "\n",
    "        return Model([noise, label], img)\n",
    "\n",
    "    \n",
    "  \n",
    "\n",
    "\n",
    "    def train(self, epochs=1000, batch_size=128, sample_interval=200, runningAvgDur=10):\n",
    "        # Load the dataset\n",
    "        X_train, y_train, X_Scaler, y_Scaler =\\\n",
    "            self.X_train, self.y_train, self.X_Scaler, self.y_Scaler\n",
    "        # X_train, y_train, X_test, y_test, X_Scaler, y_Scaler = self.load_data()\n",
    "\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        d_losses_real = []\n",
    "        d_accs_real = []\n",
    "        d_losses_fake = []\n",
    "        d_accs_fake = []\n",
    "\n",
    "        g_losses = []\n",
    "\n",
    "        startOfDur = -int(runningAvgDur)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs, labels = X_train[idx], y_train[idx]\n",
    "\n",
    "            # Sample noise as generator input\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            gen_imgs = self.generator.predict([noise, labels])\n",
    "\n",
    "            # Train the discriminator\n",
    "            d_loss_real = self.discriminator.train_on_batch([imgs, labels], valid)\n",
    "            d_loss_fake = self.discriminator.train_on_batch([gen_imgs, labels], fake)\n",
    "            # d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            # Condition on labels\n",
    "\n",
    "            sampled_indx = np.random.choice(y_train.shape[0], size=batch_size)  #np.random.randint(0, 10, batch_size).reshape(-1, 1)\n",
    "            sampled_labels = y_train[sampled_indx]\n",
    "\n",
    "            # Train the generator\n",
    "            g_loss = self.combined.train_on_batch([noise, sampled_labels], valid)\n",
    "\n",
    "\n",
    "            # Register the progress\n",
    "            d_losses_real.append(d_loss_real[0])\n",
    "            d_accs_real.append(100*d_loss_real[1])\n",
    "\n",
    "            d_losses_fake.append(d_loss_fake[0])\n",
    "            d_accs_fake.append(100*d_loss_fake[1])\n",
    "\n",
    "            g_losses.append(g_loss)\n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if (epoch > 0):\n",
    "              if (epoch % sample_interval == 0) or (epoch == epochs - 1): #  and epoch >= 3*epochs/4\n",
    "                self.sample_images(epoch)\n",
    "                print(\"%d [D loss_real: %.3f, acc_real: %.2f%%] [D loss_fake: %.3f, acc_fake: %.2f%%] [G loss: %.3f]\"\n",
    "                  % (epoch, np.average(d_losses_real[startOfDur:]),\n",
    "                     np.average(d_accs_real[startOfDur:]),\n",
    "                     np.average(d_losses_fake[startOfDur:]),\n",
    "                     np.average(d_accs_fake[startOfDur:]),\n",
    "                     np.average(g_losses[startOfDur:])))\n",
    "        \n",
    "\n",
    "        self.plot_results(d_losses_real, d_losses_fake, g_losses, d_accs_real,\n",
    "                          d_accs_fake, runningAvgDur, run_name, epochs)\n",
    "\n",
    "\n",
    "        # Save the trained models\n",
    "        self.generator.save('saved_model/'+run_name+'_generator_model.tf')\n",
    "        self.discriminator.save('saved_model/'+run_name+'_discriminator_model.tf')\n",
    "        self.combined.save('saved_model/'+run_name+'_combined_model.tf')\n",
    "\n",
    "\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        if experiment != 'FullData' and experiment != 'BestHalfAll' and experiment != 'WorstHalfAll':\n",
    "          r, c = 125, 1\n",
    "        else:\n",
    "          r, c = 4*125, 1\n",
    "\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "\n",
    "\n",
    "        sampled_labels = []\n",
    "\n",
    "        # Generate the random samples [i: LCC, j: GHG, k: WalkScore]\n",
    "        if experiment == 'WorstHalfLCC':\n",
    "          for i in np.arange(-1.0, -1.5, -0.1):\n",
    "            for j in np.arange(-1.0, 1.5, 0.5):\n",
    "              for k in np.arange(-1.0, 1.5, 0.5):\n",
    "                sampled_labels.append([i,j,k])\n",
    "        \n",
    "\n",
    "        elif experiment == 'WorstHalfCO2':\n",
    "          for i in np.arange(-1.0, 1.5, 0.5):\n",
    "            for j in np.arange(-1.0, -1.25, -0.05):\n",
    "              for k in np.arange(-1.0, 1.5, 0.5):\n",
    "                sampled_labels.append([i,j,k])\n",
    "\n",
    "        elif experiment == 'WorstHalfWalkScore':\n",
    "          for i in np.arange(-1.0, 1.5, 0.5):\n",
    "            for j in np.arange(-1.0, 1.5, 0.5):\n",
    "              for k in np.arange(1.0, 1.5, 0.1):\n",
    "                sampled_labels.append([i,j,k])\n",
    "        \n",
    "\n",
    "        elif experiment == 'FullData' or experiment == 'BestHalfAll'\\\n",
    "              or experiment == 'BestHalfAny' or experiment == 'WorstHalfAll':\n",
    "          for i in np.arange(-1.0, -1.5, -0.1):\n",
    "            for j in np.arange(-1.0, 1.5, 0.5):\n",
    "              for k in np.arange(-1.0, 1.5, 0.5):\n",
    "                sampled_labels.append([i,j,k])\n",
    "\n",
    "          for i in np.arange(-1.0, 1.5, 0.5):\n",
    "            for j in np.arange(-1.0, -1.25, -0.05):\n",
    "              for k in np.arange(-1.0, 1.5, 0.5):\n",
    "                sampled_labels.append([i,j,k])\n",
    "          \n",
    "          for i in np.arange(-1.0, 1.5, 0.5):\n",
    "            for j in np.arange(-1.0, 1.5, 0.5):\n",
    "              for k in np.arange(1.0, 1.5, 0.1):\n",
    "                sampled_labels.append([i,j,k])\n",
    "\n",
    "          for i in np.arange(-1.0, -1.5, -0.1):\n",
    "            for j in np.arange(-1.0, -1.25, -0.05):\n",
    "              for k in np.arange(1.0, 1.5, 0.1):\n",
    "                sampled_labels.append([i,j,k])\n",
    "\n",
    "\n",
    "\n",
    "        gen_imgs = self.generator.predict([noise, np.array(sampled_labels)])\n",
    "        scaled_gen_imgs = self.X_Scaler.inverse_transform(gen_imgs)\n",
    "\n",
    "        np.savetxt(\"images/\"+run_name+\"_%d.txt\" % epoch, scaled_gen_imgs)\n",
    "\n",
    "\n",
    "\n",
    "    def plot_results(self, d_losses_real, d_losses_fake, g_losses, d_accs_real,\n",
    "                     d_accs_fake, runningAvgDur, run_name, epochs):\n",
    "      # Plot the result of the model\n",
    "      plt.style.use('ggplot')\n",
    "      \n",
    "      plt.figure()\n",
    "      plt.plot(range(epochs), d_losses_real, color='red', alpha=0.1, label='Disc Loss Real')\n",
    "      d_losses_real_avg = self.moving_average(d_losses_real, runningAvgDur)\n",
    "      plt.plot(range(epochs), d_losses_real_avg, color='red', label='Disc Loss Real_Avg')\n",
    "\n",
    "      plt.plot(range(epochs), d_losses_fake, color='green', alpha=0.1, label='Disc Loss Fake')\n",
    "      d_losses_fake_avg = self.moving_average(d_losses_fake, runningAvgDur)\n",
    "      plt.plot(range(epochs), d_losses_fake_avg, color='green', label='Disc Loss Fake_Avg')\n",
    "\n",
    "      plt.plot(range(epochs), g_losses, color='blue', alpha=0.1, label='Gen Loss')\n",
    "      g_losses_avg = self.moving_average(g_losses, runningAvgDur)\n",
    "      plt.plot(range(epochs), g_losses_avg, color='blue', label='Gen Loss_Avg')\n",
    "\n",
    "      plt.title('Discriminator & Generator Loss vs Iteration Number')\n",
    "      plt.xlabel('Iteration #')\n",
    "      plt.ylabel('Loss')\n",
    "      plt.legend()\n",
    "      plt.savefig('images/'+run_name+'_Disc Loss vs Gen Loss.png', bbox_inches='tight', dpi=400)\n",
    "      plt.show()\n",
    "\n",
    "      plt.figure()\n",
    "      plt.plot(range(epochs), d_accs_real, color='red', alpha=0.1, label='Disc Acc Real')\n",
    "      d_accs_real_avg = self.moving_average(d_accs_real, runningAvgDur)\n",
    "      plt.plot(range(epochs), d_accs_real_avg, color='red', label='Disc Acc Real_Avg')\n",
    "\n",
    "      plt.plot(range(epochs), d_accs_fake, color='green', alpha=0.1, label='Disc Acc Fake')\n",
    "      d_accs_fake_avg = self.moving_average(d_accs_fake, runningAvgDur)\n",
    "      plt.plot(range(epochs), d_accs_fake_avg, color='green', label='Disc Acc Fake_Avg')\n",
    "\n",
    "      plt.title('Discriminator Accuracy vs Iteration Number')\n",
    "      plt.xlabel('Iteration #')\n",
    "      plt.ylabel('Accuracy \\%')\n",
    "      plt.legend()\n",
    "      plt.savefig('images/'+run_name+'_Disc Acc.png', bbox_inches='tight', dpi=400)\n",
    "      plt.show()\n",
    "\n",
    "\n",
    "    def moving_average(self, a, n=3):\n",
    "        # Calculate the moving average of array a with window_size n\n",
    "        ret = np.cumsum(a, dtype=float)\n",
    "        ret[n:] = ret[n:] - ret[:-n]\n",
    "        ret[:n-1] /= np.arange(1,n)\n",
    "        ret[n - 1:] /= n\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 220216,
     "status": "ok",
     "timestamp": 1610065951621,
     "user": {
      "displayName": "Pouya Rezazadeh Kalehbasti",
      "photoUrl": "",
      "userId": "12310598957606952973"
     },
     "user_tz": 480
    },
    "id": "aYJ48K-ah4og",
    "outputId": "d6b182f5-a3f8-4904-ae64-80337dc92492"
   },
   "outputs": [],
   "source": [
    "# Train the CGAN and generate outputs\n",
    "latent_dim = 3\n",
    "cgan = CGAN(X_train, y_train, X_test, y_test, X_Scaler, y_Scaler, latent_dim=latent_dim)\n",
    "# Long runs\n",
    "if long_run:\n",
    "    if experiment in ['WorstHalfLCC', 'WorstHalfCO2']: # ~155 epochs\n",
    "      cgan.train(epochs=20000, batch_size=256, sample_interval=1000, runningAvgDur=10)\n",
    "    elif experiment in ['WorstHalfWalkScore']: # ~155 epoch\n",
    "      cgan.train(epochs=24000, batch_size=256, sample_interval=1000, runningAvgDur=10) \n",
    "    elif experiment == 'FullData':# ~156 epochs\n",
    "      cgan.train(epochs=40000, batch_size=256, sample_interval=1000, runningAvgDur=10) # 256 iterations ~= 1 epoch for the full data # Tried at first batch of 64 over 10,000 epochs, it was good!\n",
    "    elif experiment == 'WorstHalfAll':# ~155 epochs\n",
    "      cgan.train(epochs=3200, batch_size=256, sample_interval=100, runningAvgDur=10) \n",
    "    elif experiment == 'BestHalfAll':# ~155 epochs\n",
    "      cgan.train(epochs=5000, batch_size=256, sample_interval=150, runningAvgDur=10) \n",
    "# Short runs:\n",
    "else:\n",
    "    if experiment in ['WorstHalfAll', 'BestHalfAll', 'FullData']:# ~16, 25 epochs\n",
    "      cgan.train(epochs=2000, batch_size=64, sample_interval=100, runningAvgDur=10) \n",
    "    elif experiment in ['WorstHalfWalkScore', 'WorstHalfLCC', 'WorstHalfCO2']: # ~1 epoch\n",
    "      cgan.train(epochs=800, batch_size=64, sample_interval=100, runningAvgDur=10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BEQw9uP1jehH"
   },
   "source": [
    "## Auxilliary: Generate more images using a saved TF model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wLbTVGDvKvnh"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "from tensorflow.keras.models import load_model\n",
    "os.chdir('./CGAN/')\n",
    "\n",
    "def sample_images(generator, epoch, y_Scaler, X_Scaler):\n",
    "  r, c = 125, 1\n",
    "  noise = np.random.normal(0, 1, (r * c, latent_dim))\n",
    "\n",
    "  sampled_labels = []\n",
    "\n",
    "  # Experiment: getting the individuals with the best LCC\n",
    "  for i in np.arange(-1.0, -1.25, -0.05):\n",
    "    for j in np.arange(-1.0, 1.5, 0.5):\n",
    "      for k in np.arange(-1.0, 1.5, 0.5):\n",
    "        sampled_labels.append([i,j,k])\n",
    "  \n",
    "\n",
    "  gen_imgs = generator.predict([noise, np.array(sampled_labels)])\n",
    "  scaled_gen_imgs = X_Scaler.inverse_transform(gen_imgs)\n",
    "\n",
    "  np.savetxt(\"images/\"+run_name+\"_%d.txt\" % epoch, scaled_gen_imgs)\n",
    "\n",
    "\n",
    "generator = load_model(filepath='./saved_model/'+run_name+'_generator_model.tf')\n",
    "sample_images(generator, epoch=1, y_Scaler=y_Scaler, X_Scaler=X_Scaler)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Evaluate the generated solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants for plotting the filtered plots\n",
    "LCC_Cutoff = 100 # k$/m2\n",
    "CO2_Cutoff = 10 # T-CO2/m2\n",
    "\n",
    "\n",
    "\n",
    "# Constants for evaluating the solutions\n",
    "Num_Sites = 4\n",
    "Num_Buildings = 4\n",
    "len_of_indiv = 11 # After adding the plant location\n",
    "\n",
    "\n",
    "LCC_Var = Num_Sites+11\n",
    "CO2_Var = Num_Sites+14\n",
    "WalkScore_Var = Num_Sites+15\n",
    "\n",
    "Num_Chillers = 3#2                           # Must be updated if new chillers are added ## NOTE: changed from 16 to 2\n",
    "Num_Engines = 6\n",
    "Building_Min = 0\n",
    "Supply_Min = 1\n",
    "\n",
    "\n",
    "Max_Supply_Temp_Heating = 95.0              # deg C\n",
    "Min_Supply_Temp_Heating = 50.0              # deg C\n",
    "Min_Summer_Heating_Reset_del_T = 0          # deg C\n",
    "Min_Winter_Cooling_Reset_del_T = 0          # deg C\n",
    "Max_Summer_Heating_Reset_del_T = 10         # deg C\n",
    "Max_Winter_Cooling_Reset_del_T = 3          # deg C\n",
    "Max_Supply_Temp_Cooling = 8.0               # deg C\n",
    "Min_Supply_Temp_Cooling = 1.0               # deg C\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load the libraries\n",
    "import sys\n",
    "import os\n",
    "Ch3_Dir = './Simulation/'\n",
    "Curr_Dir = os.getcwd()\n",
    "sys.path.append(Ch3_Dir)\n",
    "\n",
    "os.chdir(Ch3_Dir)\n",
    "from RQ3 import SupplyandDemandOptimization as analyze\n",
    "os.chdir(Curr_Dir)\n",
    "\n",
    "import numpy as np\n",
    "from Filter_Data import DF_Filter\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "## Check the validity of the inputs\n",
    "Low_Seq = []\n",
    "High_Seq = []\n",
    "for i in range(Num_Sites):\n",
    "    Low_Seq += [Building_Min]\n",
    "    High_Seq += [Num_Buildings]\n",
    "   \n",
    "# Create the Low and High Sequence values that control the location of the Central Plant\n",
    "Low_Seq += [0]\n",
    "High_Seq += [Num_Sites-1]\n",
    "\n",
    "# Create the Low and High Sequence values that control mutation in optimization for CHP engines\n",
    "Low_Seq += [Supply_Min]\n",
    "High_Seq += [Num_Engines]\n",
    "\n",
    "# Create the Low and High Sequence values that control mutation in optimization for the chillers\n",
    "Low_Seq += [Supply_Min]\n",
    "High_Seq += [Num_Chillers]\n",
    "\n",
    "# Create the Low and High Sequence values that control mutation for loop temperature and thermal reset\n",
    "Low_Seq += [Min_Supply_Temp_Heating]\n",
    "Low_Seq += [Min_Summer_Heating_Reset_del_T]\n",
    "Low_Seq += [Min_Supply_Temp_Cooling]\n",
    "Low_Seq += [Min_Winter_Cooling_Reset_del_T]\n",
    "High_Seq += [Max_Supply_Temp_Heating]\n",
    "High_Seq += [Max_Summer_Heating_Reset_del_T]\n",
    "High_Seq += [Max_Supply_Temp_Cooling]\n",
    "High_Seq += [Max_Winter_Cooling_Reset_del_T]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function for loading the filenames of the results\n",
    "def load_filenames(Experiment):\n",
    "    if Experiment == 'FullData':\n",
    "        fileNames = ['FullData_1000', 'FullData_5000', 'FullData_15000', 'FullData_39999'] \n",
    "    elif Experiment == 'WorstHalfLCC': \n",
    "        fileNames = ['WorstHalfLCC_1000', 'WorstHalfLCC_5000', 'WorstHalfLCC_10000', 'WorstHalfLCC_15000', 'WorstHalfLCC_19999']\n",
    "    elif Experiment == 'WorstHalfCO2': \n",
    "        fileNames = ['WorstHalfCO2_1000', 'WorstHalfCO2_5000', 'WorstHalfCO2_10000', 'WorstHalfCO2_15000', 'WorstHalfCO2_19999']\n",
    "    elif Experiment == 'WorstHalfWalkScore':\n",
    "        fileNames = ['WorstHalfWalkscore_1000', 'WorstHalfWalkscore_5000', 'WorstHalfWalkscore_10000', 'WorstHalfWalkscore_15000', 'WorstHalfWalkscore_23999']\n",
    "    elif Experiment == 'WorstHalfAll':\n",
    "        fileNames = ['WorstHalfAll2_100', 'WorstHalfAll2_300', 'WorstHalfAll2_500', 'WorstHalfAll2_1500', 'WorstHalfAll2_3199']\n",
    "    elif Experiment == 'BestHalfAll':\n",
    "        fileNames = ['BestHalfAll_150', 'BestHalfAll_600', 'BestHalfAll_2100', 'BestHalfAll_4050', 'BestHalfAll_4999']\n",
    "    return fileNames\n",
    "        \n",
    "        \n",
    "## Helper functions for combining the results of the long and short runs\n",
    "def results_total_finder(experiment):\n",
    "    path = './Data/'\n",
    "    files = []\n",
    "    for i in os.listdir(path):\n",
    "        if os.path.isfile(os.path.join(path,i)) and 'resultsTotal'+experiment in i:\n",
    "            files.append('./Data/'+i)\n",
    "    return files\n",
    "\n",
    "\n",
    "\n",
    "def load_combined_results(experiment):\n",
    "    allResults = None\n",
    "    for i, filename in enumerate(results_total_finder(experiment)):\n",
    "        if i == 0:\n",
    "            allResults = np.loadtxt(filename)\n",
    "        else:\n",
    "            allResults = np.append(allResults, np.loadtxt(filename), axis=0)\n",
    "        print('processed %s'%filename)\n",
    "        \n",
    "    print('Loaded %d points generated by GAN from combined results of %s'%(len(allResults), experiment))\n",
    "    return allResults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECLARE THE EXPERIMENT TYPE\n",
    "plot_only = False # If you want no evaluation and simply plotting the results, set this to True; no plots are made when this is False\n",
    "stats_and_plot = False # If you want statistical info to be printed out while plot_only is True, set this to True\n",
    "aggregate_results = False # If you want the results of the long and short runs to get combined, set this to be True\n",
    "verbose = True # If you want full outputs to be printed out, set this to True\n",
    "\n",
    "for Experiment in ['WorstHalfCO2', 'WorstHalfLCC', 'WorstHalfWalkScore',\n",
    "                    'WorstHalfAll', 'BestHalfAll', 'FullData']: ## MODIFY THIS TO RUN DIFFERENT EXPERIMENTS\n",
    "    plt.close('all')\n",
    "    # Load the proper set of filenames based on the experiment\n",
    "    fileNames = load_filenames(Experiment)\n",
    "\n",
    "    \n",
    "    # Read in the data generated by GAN or load the already processed dataset\n",
    "    if not plot_only:\n",
    "        if not os.path.exists('./Data/rebuilt_'+fileNames[0]+'.txt'):\n",
    "            # If more than one filename provided, combine them and save them as one\n",
    "            loadedData = np.round(np.loadtxt('./Data/'+fileNames[0]+'.txt'))\n",
    "            for name in fileNames[1:]:\n",
    "                loadedData = np.append(loadedData, np.round(np.loadtxt('./Data/'+name+'.txt')), axis=0)\n",
    "                \n",
    "            np.savetxt('./Data/rebuilt_'+fileNames[0]+'.txt', loadedData)\n",
    "            print('Saved rebuilt data generated by GAN into %s'%('./Data/rebuilt_'+fileNames[0]+'.txt'))\n",
    "        else:\n",
    "            loadedData = np.loadtxt('./Data/rebuilt_'+fileNames[0]+'.txt')\n",
    "            print('Loaded rebuilt data generated by GAN from %s'%('./Data/rebuilt_'+fileNames[0]+'.txt'))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Evaluate the individuals or load the already-evaluated individuals\n",
    "    if not aggregate_results and not os.path.exists('./Data/resultsTotal'+fileNames[0]+'.txt'):\n",
    "        # Evaluate the individuals\n",
    "        resultsTotal = []\n",
    "        correctedIndivs = 0 # Number of individuals corrected (meaning they were outside the allowed boundary)\n",
    "        ignoredIndivs = 0 # Number of ignored individuals\n",
    "    \n",
    "        for indiv in tqdm(loadedData):\n",
    "            indiv = list(indiv)\n",
    "            # Fit the generated individual inside the High_Seq - Low_Seq range\n",
    "            oldIndiv = indiv\n",
    "            \n",
    "            # Find the plant location and insert it into the individual's DNA\n",
    "            plantLoc = None\n",
    "            for i in range(4):\n",
    "                if indiv[i] >= Num_Buildings: # Found potential plant location\n",
    "                    if plantLoc == None: # First time finding such location\n",
    "                        plantLoc = i\n",
    "                        indiv[i] = 0 # Setting the building variable in plant location to zero\n",
    "                    else: # Already found the plant location\n",
    "                        indiv[i] = 0 # Invalid plant location, removing the building in its place\n",
    "                        \n",
    "            if plantLoc == None: # Not found any plant locations!\n",
    "                ignoredIndivs += 1\n",
    "                continue\n",
    "            else: # Insert plant location into the individual's DNA\n",
    "                indiv.insert(Num_Sites, plantLoc)\n",
    "            \n",
    "            # Check the validity of the DNA and round to the nearest valid number\n",
    "            indiv = list(np.minimum(np.maximum(indiv, Low_Seq), High_Seq))\n",
    "            \n",
    "            # Check if there are any buildings in the development\n",
    "            if np.sum(indiv[:4]) == 0:\n",
    "                ignoredIndivs += 1\n",
    "                continue\n",
    "            \n",
    "            # Register if the individual has been corrected\n",
    "            if indiv != oldIndiv:\n",
    "                correctedIndivs += 1\n",
    "    \n",
    "            results = analyze(indiv, solver='gurobi')\n",
    "    \n",
    "            # Skip trivial cases\n",
    "            if float('inf') in results[0] or -float('inf') in results[0]: \n",
    "                ignoredIndivs += 1\n",
    "                continue\n",
    "    \n",
    "            [LCC_per_GFA, CO2_per_GFA, WalkScore,] = results[0]\n",
    "            if LCC_per_GFA != float('inf'):\n",
    "                indiv.extend(results[0])\n",
    "                resultsTotal.append(indiv)\n",
    "    \n",
    "        print('\\n\\n++++++++++++++++++++++++++\\nNumber of corrected individuals: %d out of %d'%(correctedIndivs, len(loadedData)))\n",
    "        print('Number of ignored individuals: %d out of %d'%(ignoredIndivs, len(loadedData)))\n",
    "    \n",
    "        np.savetxt('./Data/resultsTotal'+fileNames[0]+'.txt', resultsTotal)\n",
    "        print('Saved points generated by GAN and their associated results into %s'%('./Data/resultsTotal'+fileNames[0]+'.txt'))\n",
    "    \n",
    "    else:\n",
    "        if not aggregate_results:\n",
    "            resultsTotal = np.loadtxt('./Data/resultsTotal'+fileNames[0]+'.txt')\n",
    "            print('Loaded points generated by GAN from %s'%('./Data/resultsTotal'+fileNames[0]+'.txt'))\n",
    "        else:\n",
    "            resultsTotal = load_combined_results(Experiment)\n",
    "\n",
    "    resultsTotal = np.array(resultsTotal)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if verbose: print('loading data generated by the optimization algorithm')\n",
    "    filename = './IILP_Toy_Optimization_TestRuns.txt'\n",
    "    DFName = 'Initial'\n",
    "    if plot_only or stats_and_plot:\n",
    "        DF = DF_Filter(filename, experiment=Experiment, verbose=0)\n",
    "    else:\n",
    "        DF = DF_Filter(filename, experiment=Experiment)#, verbose=0)\n",
    "    \n",
    "    \n",
    "    plt.style.use('ggplot')\n",
    "    colors_rb = {DFName:'r'}\n",
    "    \n",
    "    #############################################\n",
    "    \n",
    "    if plot_only or stats_and_plot:\n",
    "        if verbose: print('\\nPlotting!\\n')\n",
    "    \n",
    "    \n",
    "        # LCC vs CO2\n",
    "        if verbose: print('Plotting LCC vs CO2')\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.scatter(x=DF[LCC_Var]/10**3,y=DF[CO2_Var], label=DFName, s=100, alpha=1, c=colors_rb[DFName], marker='x')\n",
    "        plt.scatter(x=resultsTotal[:,len_of_indiv+0]/10**3, y=resultsTotal[:,len_of_indiv+1], label='C-GAN', s=150, alpha=1, c='blue', marker='+')\n",
    "        \n",
    "        plt.xlabel(r'LCC (k\\$/$m^2$)')\n",
    "        plt.ylabel(r'GHG (T-$CO_{2e}$/$m^2$)')\n",
    "        plt.legend()\n",
    "        plt.savefig('./Figures/All_CO2_vs_LCC'+fileNames[0]+'.png', dpi=400, bbox_inches='tight')\n",
    "        \n",
    "        \n",
    "        \n",
    "        # LCC vs Walkscore\n",
    "        if verbose: print('Plotting LCC vs Walkscore')\n",
    "        plt.figure(figsize=(10,5))\n",
    "        # DF = DF[DF[LCC_Var]/10**3 <= 500]\n",
    "        # DF = DF[DF[Walk] <= 100] # Observation: with a less than $500k/m2 of LCC, no designs have CO2/m2 higher than 5 T-CO2/m2\n",
    "        plt.scatter(x=DF[LCC_Var]/10**3,y=DF[WalkScore_Var], label=DFName, s=100, alpha=1, c=colors_rb[DFName], marker='x')\n",
    "        plt.scatter(x=resultsTotal[:,len_of_indiv+0]/10**3,y=resultsTotal[:,len_of_indiv+2], label='C-GAN', s=150, alpha=1, c='blue', marker='+')\n",
    "        \n",
    "        plt.xlabel(r'LCC (k\\$/$m^2$)')\n",
    "        plt.ylabel(r'Walkscore')\n",
    "        plt.legend()\n",
    "        plt.savefig('./Figures/All_Walkscore_vs_LCC'+fileNames[0]+'.png', dpi=400, bbox_inches='tight')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    if not plot_only or stats_and_plot:\n",
    "        ## FIND DUPLICATE ELEMENTS B/W The original and the synthetic individuals\n",
    "        if verbose: print('\\n\\n+++++\\nFind duplicated elements b/w the original and the synthetic individuals')\n",
    "        original_data = np.array(DF.iloc[:,:13])\n",
    "        original_data = np.concatenate((original_data[:,:7], original_data[:,9:]),axis=1)\n",
    "        list_original_data = [list(item) for item in original_data]\n",
    "        \n",
    "        \n",
    "        synth_data = resultsTotal[:,:11]\n",
    "        list_synth_data = [list(item) for item in synth_data]\n",
    "        \n",
    "        duplicates = []\n",
    "        for i in range(len(synth_data)):\n",
    "            if list_synth_data[i] in list_original_data:\n",
    "                duplicates.append(i)\n",
    "                \n",
    "        if verbose: print('Number of elements shared between the original and the synthetic data: %d'%len(duplicates))\n",
    "        # if duplicates:\n",
    "        #     print('List of elements shared between the original and the synthetic data (indices defined): %d'%len(duplicates))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "        # Check the improvement in targeted objective function compared to the training data\n",
    "        if Experiment == 'WorstHalfLCC' or Experiment == 'BestHalfLCC':\n",
    "            lowest_generated = np.min(resultsTotal[:,len_of_indiv+0])\n",
    "            print('lowest LCC in generated individuals: %.2f'%lowest_generated)\n",
    "            print('--> Percent improvement: %.1f%%'%((lowest_generated-np.min(DF[LCC_Var]))/np.min(DF[LCC_Var])*100))\n",
    "        elif Experiment == 'WorstHalfCO2':\n",
    "            lowest_generated = np.min(resultsTotal[:,len_of_indiv+1])\n",
    "            print('Lowest CO2 in generated data: %.2f'%lowest_generated)\n",
    "            print('--> Percent improvement: %.1f%%'%((np.min(DF[CO2_Var])-lowest_generated)/np.min(DF[CO2_Var])*100))\n",
    "        elif Experiment == 'WorstHalfWalkScore':\n",
    "            highest_generated = np.max(resultsTotal[:,len_of_indiv+2])\n",
    "            print('Highest Walkscore in generated data: %.2f'%highest_generated)\n",
    "            print('--> Percent improvement: %.1f%%'%((highest_generated - np.max(DF[WalkScore_Var]))/np.max(DF[WalkScore_Var])*100))\n",
    "        elif Experiment == 'WorstHalfAll' or Experiment == 'FullData' or Experiment == 'BestHalfAll' or Experiment == 'BestHalfAny':\n",
    "            lowest_generated_LCC = np.min(resultsTotal[:,len_of_indiv+0])\n",
    "            lowest_generated_CO2 = np.min(resultsTotal[:,len_of_indiv+1])\n",
    "            highest_generated_WalkScore = np.max(resultsTotal[:,len_of_indiv+2])\n",
    "            print('lowest LCC in generated individuals: %.2f'%lowest_generated_LCC)\n",
    "            print('Lowest CO2 in generated data: %.2f'%lowest_generated_CO2)\n",
    "            print('Highest Walkscore in generated data: %.2f'%highest_generated_WalkScore)\n",
    "            print('--> Percent improvement in LCC: %.1f%%'%((lowest_generated_LCC-np.min(DF[LCC_Var]))/np.min(DF[LCC_Var])*100))\n",
    "            print('--> Percent improvement in CO2: %.1f%%'%((np.min(DF[CO2_Var])-lowest_generated_CO2)/np.min(DF[CO2_Var])*100))\n",
    "            print('--> Percent improvement in WalkScore: %.1f%%'%((highest_generated_WalkScore - np.max(DF[WalkScore_Var]))/np.max(DF[WalkScore_Var])*100))\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if plot_only or stats_and_plot:\n",
    "        # LCC vs CO2 (Filtered below the cutoff thresholds)\n",
    "        ## Filter dataframes based on max values of CO2 and LCC\n",
    "        if verbose: print('++++\\nFiltering DF and resultsTotal based on cutoffs (LCC_Cutoff: %d, CO2_Cutoff: %d)'%(LCC_Cutoff, CO2_Cutoff))\n",
    "        DF = DF[DF[LCC_Var]/10**3 <= LCC_Cutoff]\n",
    "        DF = DF[DF[CO2_Var] <= CO2_Cutoff] # Observation: with a less than $500k/m2 of LCC, no designs have CO2/m2 higher than 5 T-CO2/m2\n",
    "        resultsTotal = resultsTotal[resultsTotal[:,len_of_indiv+0]/10**3 <= LCC_Cutoff]\n",
    "        resultsTotal = resultsTotal[resultsTotal[:,len_of_indiv+1] <= CO2_Cutoff]\n",
    "        \n",
    "        \n",
    "        \n",
    "        if verbose: print('Plotting LCC vs CO2')\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.scatter(x=DF[LCC_Var]/10**3,y=DF[CO2_Var], label=DFName, s=100, alpha=1, c=colors_rb[DFName], marker='x')\n",
    "        plt.scatter(x=resultsTotal[:,len_of_indiv+0]/10**3, y=resultsTotal[:,len_of_indiv+1], label='C-GAN', s=150, alpha=1, c='blue', marker='+')\n",
    "        \n",
    "        plt.xlabel(r'LCC (k\\$/$m^2$)')\n",
    "        plt.ylabel(r'GHG (T-$CO_{2e}$/$m^2$)')\n",
    "        plt.legend()\n",
    "        plt.savefig('./Figures/CO2_vs_LCC'+fileNames[0]+'.png', dpi=400, bbox_inches='tight')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # LCC vs Walkscore (Filtered below the cutoff thresholds)\n",
    "        if verbose: print('Plotting LCC vs Walkscore')\n",
    "        plt.figure(figsize=(10,5))\n",
    "        plt.scatter(x=DF[LCC_Var]/10**3,y=DF[WalkScore_Var], label=DFName, s=100, alpha=1, c=colors_rb[DFName], marker='x')\n",
    "        plt.scatter(x=resultsTotal[:,len_of_indiv+0]/10**3,y=resultsTotal[:,len_of_indiv+2], label='C-GAN', s=150, alpha=1, c='blue', marker='+')\n",
    "        \n",
    "        plt.xlabel(r'LCC (k\\$/$m^2$)')\n",
    "        plt.ylabel(r'Walkscore')\n",
    "        plt.legend()\n",
    "        plt.savefig('./Figures/Walkscore_vs_LCC'+fileNames[0]+'.png', dpi=400, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Calculate the changes in hypervolume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "ref_point = [1.0, 1.0, 1.0] # Reference point for calculating the hypervolume\n",
    "\n",
    "\n",
    "## Labels for the generated dataset\n",
    "LCC_Var_Gen = len_of_indiv+0\n",
    "CO2_Var_Gen = len_of_indiv+1\n",
    "WalkScore_Var_Gen = len_of_indiv+2\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "\n",
    "# DECLARE THE EXPERIMENT TYPE\n",
    "aggregate_results = True\n",
    "for Experiment in ['WorstHalfCO2', 'WorstHalfLCC', 'WorstHalfWalkScore',\n",
    "                    'WorstHalfAll', 'BestHalfAll', 'FullData']: ## MODIFY THIS TO RUN DIFFERENT EXPERIMENTS\n",
    "    print('\\n+++++++++++++++++')\n",
    "    print('Results for the experiment %s:\\n'%Experiment)\n",
    "    \n",
    "    # Load the proper set of filenames based on the experiment\n",
    "    fileNames = load_filenames(Experiment)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Load the datasets\n",
    "    if not aggregate_results:\n",
    "        resultsTotal = pd.DataFrame(np.loadtxt('./Data/resultsTotal'+fileNames[0]+'.txt'))\n",
    "        print('Loaded points generated by GAN from %s'%('./Data/resultsTotal'+fileNames[0]+'.txt'))\n",
    "    else:\n",
    "        resultsTotal = pd.DataFrame(load_combined_results(Experiment))\n",
    "\n",
    "    filename = './IILP_Toy_Optimization_TestRuns.txt'\n",
    "    DF = DF_Filter(filename, experiment=Experiment, verbose=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Calculate the hypervolume\n",
    "    ## modify the input parameters to make them suitable for calculating the HV w.r.t. the reference point (1,1,1)\n",
    "    maxLCC = max(np.max(DF[LCC_Var]), np.max(resultsTotal[LCC_Var_Gen]))\n",
    "    maxCO2 = max(np.max(DF[CO2_Var]), np.max(resultsTotal[CO2_Var_Gen]))\n",
    "    maxWalkScore = max(np.max(DF[WalkScore_Var]), np.max(resultsTotal[WalkScore_Var_Gen]))\n",
    "    \n",
    "    minLCC = min(np.min(DF[LCC_Var]), np.min(resultsTotal[LCC_Var_Gen]))\n",
    "    minCO2 = min(np.min(DF[CO2_Var]), np.min(resultsTotal[CO2_Var_Gen]))\n",
    "    minWalkScore = min(np.min(DF[WalkScore_Var]), np.min(resultsTotal[WalkScore_Var_Gen]))\n",
    "    \n",
    "    \n",
    "    ## Reverse the walkscore column\n",
    "    def reverse_data(DF, column, maxValue): # Subtract each value from the given max value in a column of a dataframe/np.array\n",
    "        DF[column] = maxValue - DF[column]\n",
    "        \n",
    "    reverse_data(DF, WalkScore_Var, maxWalkScore)\n",
    "    reverse_data(resultsTotal, WalkScore_Var_Gen, maxWalkScore)\n",
    "    \n",
    "    \n",
    "    ## Normalize the data to 0 to 1\n",
    "    def normalize_data(DF, column, minValue, maxValue):\n",
    "        DF[column] = (DF[column] - minValue)/(maxValue - minValue)\n",
    "    \n",
    "    normalize_data(DF, LCC_Var, minLCC, maxLCC)\n",
    "    normalize_data(DF, CO2_Var, minCO2, maxCO2)\n",
    "    normalize_data(DF, WalkScore_Var, minWalkScore, maxWalkScore)\n",
    "    \n",
    "    normalize_data(resultsTotal, LCC_Var_Gen, minLCC, maxLCC)\n",
    "    normalize_data(resultsTotal, CO2_Var_Gen, minCO2, maxCO2)\n",
    "    normalize_data(resultsTotal, WalkScore_Var_Gen, minWalkScore, maxWalkScore)\n",
    "    \n",
    "    \n",
    "    \n",
    "    ## Calculate the hv\n",
    "    from pymoo.factory import get_performance_indicator\n",
    "    hv = get_performance_indicator(\"hv\", ref_point=np.array(ref_point))\n",
    "    \n",
    "    \n",
    "    ## GIVES MEMORY ERROR IF USED DIRECTLY ##\n",
    "    array1 = np.array(DF[[LCC_Var, CO2_Var, WalkScore_Var]])\n",
    "    \n",
    "    \n",
    "    array2 = np.array(resultsTotal[[LCC_Var_Gen, CO2_Var_Gen, WalkScore_Var_Gen]])\n",
    "    generatedArea = hv.calc(array2)\n",
    "    \n",
    "    \n",
    "    originalAreas = []\n",
    "    # prevArr = None\n",
    "    Num_Samplings = int(len(array1)/len(array2))+1\n",
    "    for i in range(Num_Samplings):\n",
    "        choices = np.random.randint(low=0, high=len(array1), size=len(array2))\n",
    "        array1_2 = np.array(DF[[LCC_Var, CO2_Var, WalkScore_Var]])[choices, :]\n",
    "        originalAreas.append(hv.calc(array1_2))\n",
    "    \n",
    "    meanOriginalArea = np.mean(originalAreas)\n",
    "    maxOriginalArea = np.max(originalAreas)\n",
    "    \n",
    "      \n",
    "    print('Maximum generated hv:%.2e'%generatedArea)\n",
    "    print('Maximum original hv:%.2e'%maxOriginalArea)\n",
    "    print('Mean original hv:%.2e'%meanOriginalArea)\n",
    "    if generatedArea > meanOriginalArea:\n",
    "        print('Generated solutions have on average a hv %.2f%% larger than the original solutions'%((generatedArea - meanOriginalArea)/meanOriginalArea*100))\n",
    "    else:\n",
    "        print('Original solutions have on average a hv %.2f%% larger than the generated solutions'%((meanOriginalArea - generatedArea)/meanOriginalArea*100))\n",
    "        \n",
    "        \n",
    "    if generatedArea > maxOriginalArea:\n",
    "        print('Generated solutions have a hv %.2f%% larger than the best of original solutions'%((generatedArea - maxOriginalArea)/maxOriginalArea*100))\n",
    "    else:\n",
    "        print('Original solutions have at best a hv %.2f%% larger than the generated solutions'%((maxOriginalArea - generatedArea)/maxOriginalArea*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOokCxdMrHScj2+TyNMig+y",
   "collapsed_sections": [
    "vPIuU3oyiEAD"
   ],
   "mount_file_id": "1mNn1QDM_n0hf1LB411iH0CVvJDKDjjfj",
   "name": "Copy of CGAN.ipynb",
   "provenance": [
    {
     "file_id": "1SmtL8WzdAmmR3NKShjFKTpQb-slPPkWJ",
     "timestamp": 1603313278436
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
